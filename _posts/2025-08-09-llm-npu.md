---
title: "Fast On-device LLM Inference with NPUs"
date: 2025-08-09 14:18:00 +0900
categories: [Papers, Inference Acceleration]
tags: [cs, ai, on-device ai]
math: true
---

해당 논문([링크](https://dl.acm.org/doi/pdf/10.1145/3669940.3707239))은 ASPLOS 2025에 publish된 논문으로, 이전에 정리한 HeteroLLM 다음으로 읽어보게 되었다. HeteroLLM은 method 측면에서는 흥미로웠지만 논리적인 허점이 존재하고, 학회에 accpet되어 있지도 않았으며, 코드도 배포되어 있지 않았다.
반면 이 논문은 최근 메이저 학회에 accept되었으면서 코드([UbiquitousLearning/mllm](https://github.com/UbiquitousLearning/mllm?tab=readme-ov-file))도 배포되어 있기 때문에 교수님께서 다음으로 읽어볼 논문으로 추천해주셨다. 심지어 github 외에도, 해당 코드에 대한 활용 가이드([mllm_website](https://ubiquitouslearning.github.io/mllm_website/#features))도 제공하고 있다.

## Abstract

## Introduction

## Background

## llm.npu Design

## Implementation & Evaluation

## Discussion & Future Work

## 결론


