---
title: "Fast On-device LLM Inference with NPUs"
date: 2025-08-09 14:18:00 +0900
categories: [Papers, Inference Acceleration]
tags: [cs, ai, on-device ai]
math: true
---

해당 논문([링크](https://dl.acm.org/doi/pdf/10.1145/3669940.3707239))은 ASPLOS 2025에 publish된 논문으로, 이전에 정리한 HeteroLLM 다음으로 읽어보게 되었다. HeteroLLM은 method 측면에서는 흥미로웠지만 논리적인 허점이 존재하고, 학회에 accpet되어 있지도 않았으며, 코드도 배포되어 있지 않았다.
반면 이 논문은 최근 메이저 학회에 accept되었으면서 코드도 배포되어 있기 때문에 교수님께서 다음으로 읽어볼 논문으로 추천해주셨다. 

<!-- [UbiquitousLearning/mllm](https://github.com/UbiquitousLearning/mllm?tab=readme-ov-file)
심지어 github 외에도, 해당 코드에 대한 활용 가이드([mllm_website](https://ubiquitouslearning.github.io/mllm_website/#features))도 제공하고 있다. 

코드가 정확히 어디있는지 모르겠다. mllm 코드에 포함되어 있나?
논문 앞쪽에는 https://github.com/UbiquitousLearning/mllm 에 있다고 하고,
appendix에 보면 https://zenodo.org/records/14392760 에 artifact를 제공하고 있다. 여기에도 코드가 있는 거 같은데, mllm과 유사해 보인다. 이게 mllm을 수정한 버전인지, 아니면 mllm 자체인지는 확인해봐야 한다.
-->

## Abstract

privacy 문제와 mobile-sized model들의 등장에 따라, LLM의 on-device inference는 그 중요성이 커지고 있다. 하지만 특히 prefill phase에서의 inference latency가 그 병목으로 존재하는데, 본 논문에서 제안하는 llm.npu에서는 NPU offloading을 활용해 prefill latency를 줄인다. llm.npu는 prompt와 모델에 대해 prompt level, tensor level, block level 각각에 대한 re-constructing으로 효율적인 이를 달성한다.

## Introduction

### 해당 연구의 필요성

Qwen2와 Phi3와 같이 적은 parameter를 가진 mobile-sized 모델들이 좋은 성능을 보이지만, 높은 inference latency는 여전히 큰 장애물로 남아있다. 실험을 통해 확인해보면 end-to-end latency 중 prefill phase가 차지하는 비중이 94.4% ~ 98.9% 정도이고, 이는 LLM이 처리하는 personalized task(또는 단순 task)의 context가 종종 매우 길기 때문이다. 하지만 기존의 연구들은 decoding phase에서의 text generation을 가속화하는 기법들을 주로 다뤘기 때문에(ex. speculative decoding 등), 본 연구에서는 on-device LLM의 prefill speed를 확보하는 것을 목표로 한다.

LLM의 prefill phase는 computation-bound하므로, mobile CPU/GPU의 제한된 computing power가 prefill phase에서의 병목이 될 수 있다. 이에 따라 integer vector operation에 효율적이고, energy-efficient하고, workload contention이 비교적 적은 NPU를 사용하는 것이 효과적일 수 있다. 하지만 NPU가 LLM inference에 대해 여러 장점을 가지고 있음에도, 상용 mobile NPU에서 LLM inference를 지원하는 시스템은 존재하지 않는다. 그 이유는 아래와 같다.

<!-- matrix-matrix multiplication에서도 NPU가 GPU를 능가할 수 있나? 아니면 vector-matrix multiplication에서만 더 뛰어난건가? -->

- Costly preparation for variable-length prompts

    현재의 mobile NPU는 static computation graph만을 지원한다. 즉, static shape의 prompt만을 처리할 수 있는데, LLM prompt는 dynamic하다는 문제가 있다. 이에 따라 새로운 shape의 prompt가 입력될 때마다 매번 computation graph를 새로 계산하거나 단순히 padding을 추가해 연산할 수 있는데, 두 방식 모두 비효율적이고 비용이 높다.

<!-- mobile NPU가 static graph만을 지원하는 이유를 아케텍처적인 과점에서 다시 찾아보자. -->

- Mismatch between LLM quantization algorithms and mobile NPU design

    outlier activation의 존재 때문에 SOTA LLM들은 per-group quantization을 활용한다. 즉, activation과 weight를 여러 group으로 나누고, group 별 scaling을 통해 outlier가 다른 group에 영향을 주지 못하도록 한다. 하지만 NPU는 per-group MatMul(Matrix Multiplcation)을 수행하지 못하고, 각 group별로 tensor를 쪼개서 각각에 대한 MatMul을 수행한 후 float sum operation으로 더해야 한다. 이에 따라 10.7x만큼의 performance overhead가 존재한다.

<!-- outlier activation 때문에 group quantization을 쓴다? 이거에 대한 더 자세한 설명을 찾아보자. -->
<!-- NPU가 group 단위 연산이 불가능한 것은, NPU는 그 아키텍처적인 특성상 단순 곱만을 수행하므로 그룹별 연산을 수행할 순 없기 때문인가.. -->

- Floating point opeerations cannot be eliminated

    LLM은 layer normalizatoin, attention 등의 연산을 수행해야 하므로 accuracy 저하 없이 integer-only execution으로 quantization하기 어렵다. 하지만 mobile NPU는 일반적으로 integer MatMul은 잘 가속화하지만, floating point MatMul은 잘 하지 못한다.

<!-- mobile NPU는 왜 integer에 적합하고 floating point 연산은 잘 못하는 것인가? 그냥 아키텍처가 그렇게 생겨먹은건가.. 더 찾아보자. -->
<!-- 상용 NPU 스펙을 봐보는 것도 좋겠다. -->

### llm.npu

논문에서 제안하는 llm.npu는 효율적인 NPU offloading을 구현한 최초의 LLM inference system으로, decoder-only LLM(ex. llama, gpt 등)에 대해 prefill latency와 energy consumption을 줄이는 것을 그 목표로 한다. llm.npu의 핵심 아이디어는 NPU를 활용해 integer computation을 가속화하고, 필수적인 floating point 연산은 CPU/GPU에서 수행하여 accuracy를 확보하는 것이다. 이를 위해 llm.npu는 아래와 같이 promp와 모델에 대해 prompt/tensor/block level로 re-constructing을 수행한다.

- Chunck-sharing graphs (prompt level re-constructing)

    variable-length prompt들을 다수의 fixed-sized chunk들로 나눠 연산한다. 이때 chunck는 그 크기에 따라 대응되는 pre-built된 subgraph를 가지고 있어 NPU에서 연산이 가능하다. 특히 각 chunk는 chunk-level causal dependency를 고려하여 실행되는데, 이는 선행 token들만을 고려해 attention 연산을 수행하는 decoder의 insight를 활용한 것으로, data dependancy를 유지한다.

    pre-built chunk graph들을 동시에 load하는 것은 memory overhead가 크므로, prompt size에 관련 없는 연산(ex. FFN)들은 각 chunk의 실행에서 공유하도록 한다.

<!-- 어떻게 causal하게 처리한다는 것인가? -->
<!-- memory에 load하는 것을 말하는 건가? -->
<!-- 이게 왜 prompt size에 관련 없다는 것인가? -->

- Shadow outlier execution (tensor level re-constructing)

    outlier들은 추출해 CPU/GPU에서 병렬적으로 연산하도록 한다.

    이때 outlier들에 대한 MatMul을 위해 weight를 CPU memory에 복사함에 따라 증가하는 memory footprint와, CPU/GPU와 NPU 사이의 synchronization overhead가 발생한다. 이에 따라 outlier들이 특정 channel position에서 주로 등장한다는 관찰을 기반으로, 해당 channel들에 해당하는 weight들만 memory에 올려두고 나머지는 disk에 저장했다가 on demand로 가져오도록 한다. 또한 outlier의 importance를 측정하고 그 값에 따라 prune하여 synchronization overhead를 줄인다.

    이때 outlier들은 굉장히 sparse하다고(0.1% ~ 0.3%) 한다.

<!-- 기본이 NPU이고, NPU에서의 outlier들을 추출해 GPU에서 연산하는 것? -->
<!-- CPU memory에 왜 복사하지? 어차피 다 하나의 메모리 안에 있는 거 아닌가? 병렬적으로 처리하기 때문인가? 모델 전체를 올려놓는 게 아니므로.. -->
<!-- memory footprint는 메모리 사용량을 말하는 거 같다. -->
<!-- 여기에서 channel은 뭘 말하는 건가? -->
<!-- 이걸 써서 group quantization을 하지 않는건가..? -->

- Out-of-order subgraph execution (block level re-constructing)

    transformer block들을 out-of-order로 CPU/GPU와 NPU에 scheduling한다. 다시 말해, 각 chunk에 대한 subgraph들을 기존 prompt에서의 순서로 딱 맞춰 실행하는 대신, out-of-order로 scheduling한다. 
    
    이때 subgraph들에 대한 최적의 실행 순서를 찾는 것은 NP-hard이므로, llm.npu는 microsecond-level의 online scheduling 알고리즘을 사용한다. 이는 NPU의 연산이 더 heavy하며, critical path를 구성하기 때문에, 각 subgraph의 실행이 가지는 latency를 줄이는 것보다는 NPU stall이 적어지도록 하는 것을 우선적으로 선택한다. 즉, 이 기법은 parallel processing을 극대화하는 것보다는, CPU/GPU의 float 연산에 따른 bubble을 줄여 CPU/GPU의 영향을 없애면서 NPU 활용을 늘리는 데에 목적이 있다.

<!-- critical path란? -->
<!-- NP-hard란? -->

llm.npu는 MLLM과 QNN을 활용해 구현되었고, 여러 모델과 device, benchmark에 대해 실험한 결과 inference accuracy를 유지하면서 prefill latency과 energy consumption 측면에서 모든 baseline에 대해 성능 향상을 보였다고 한다.

## Background

### On-device LLM Inference Analysis

Opt, Gemma, Qwen, phi 등 on-device에서 돌아갈 수 있을만한 여러 lightweight LLM들이 개발되어 있지만, inference latency는 여전히 무시하지 못할 수준이다. 논문에서는 이 inference latency에 대한 관찰을 수행했다. 지금까지 decoding phase에 대한 연구가 활발히 진행된 것에 반해, 아래 그래프에서와 같은 전형적인 mobile application들에서 prefill phase가 병목인 것을 알 수 있다. 수준의 차이는 있지만 CPU를 활용하는 경우와 GPU를 활용하는 경우 모두에서 prefill latency의 비중이 높았고, prompt length가 길어질수록 그 비율이 높아졌다고 한다.

![](/assets/img/posts/2025-08-09-llm-npu/prefill latency.png){: width="450" }

이런 application들에서 prefill phase가 병목이 되는 이유는 아래와 같다.

- mobile CPU/GPU는 application logic이나 redering task를 처리하는 것을 주 목적으로 하므로, cloud GPU보다 parallelism capability가 낮다.

- mobile LLM task는 personalized, context-aware generation을 위해 long prompt에 대한 처리를 수행하므로 prefill latency가 큰 반면, output length가 대체로 짧아 decoding latency가 적다. 예를 들어, automated email reply의 경우 다양한 사용자 데이터를 고려해야 하고, UI automation의 경우 HTML 파일들과 사용자 명령을 고려해야 한다. 

### Mobile NPUs

이런 prefill latency를 줄이기 위해, 저자는 mobile SoC에 포함된 NPU(Neural Processing Unit)를 활용하는 것을 제안한다. NPU는 특히 INT8과 같은 integer-based matrix multiplication 연산에 적합하다고 한다. 아래는 주요 제조사들이 제공하는 NPU 목록이다.

![](/assets/img/posts/2025-08-09-llm-npu/npu vendors.png){: width="450" }



<!-- NPU가 FP가 아니라 INT 연산에서 강점을 가지는 이유? 아키텍처적인 이유가 있을 것 같다. -->
<!-- 여기서는 heteroLLM과 달리 INT 연산이 훨씬 좋다고 한다. INT 연산이 빠른 건 알겠는데, 정확도는 보여주고 있지 않다. -->


### Gap between LLMs and Mobile NPUs



## llm.npu Design

## Implementation & Evaluation

## Discussion & Future Work

## 결론


