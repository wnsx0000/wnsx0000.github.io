---
title: "[논문 정리] Fast On-device LLM Inference with NPUs"
date: 2025-08-09 14:18:00 +0900
categories: [Papers, Inference Acceleration]
tags: [cs, ai, on-device ai]
math: true
---

해당 논문([링크](https://dl.acm.org/doi/pdf/10.1145/3669940.3707239))은 ASPLOS 2025에 publish된 논문으로, 이전에 정리한 HeteroLLM 다음으로 읽어보게 되었다. HeteroLLM은 method 측면에서는 흥미로웠지만 논리적인 허점이 존재하고, 학회에 accpet되어 있지도 않았으며, 코드도 배포되어 있지 않았다.
반면 이 논문은 최근 메이저 학회에 accept되었으면서 코드도 배포되어 있기 때문에 교수님께서 다음으로 읽어볼 논문으로 추천해주셨다. 

<!-- [UbiquitousLearning/mllm](https://github.com/UbiquitousLearning/mllm?tab=readme-ov-file)
심지어 github 외에도, 해당 코드에 대한 활용 가이드([mllm_website](https://ubiquitouslearning.github.io/mllm_website/#features))도 제공하고 있다. 

코드가 정확히 어디있는지 모르겠다. mllm 코드에 포함되어 있나?
논문 앞쪽에는 https://github.com/UbiquitousLearning/mllm 에 있다고 하고,
appendix에 보면 https://zenodo.org/records/14392760 에 artifact를 제공하고 있다. 여기에도 코드가 있는 거 같은데, mllm과 유사해 보인다. 이게 mllm을 수정한 버전인지, 아니면 mllm 자체인지는 확인해봐야 한다.
-->

## Abstract

privacy 문제와 mobile-sized model들의 등장에 따라, LLM의 on-device inference는 그 중요성이 커지고 있다. 하지만 특히 prefill phase에서의 inference latency가 그 병목으로 존재하는데, 본 논문에서 제안하는 llm.npu에서는 NPU offloading을 활용해 prefill latency를 줄인다. llm.npu는 prompt와 모델에 대해 prompt level, tensor level, block level 각각에 대한 re-constructing으로 효율적인 이를 달성한다.

## Introduction

### 해당 연구의 필요성

Qwen2와 Phi3와 같이 적은 parameter를 가진 mobile-sized 모델들이 좋은 성능을 보이지만, 높은 inference latency는 여전히 큰 장애물로 남아있다. 실험을 통해 확인해보면 end-to-end latency 중 prefill phase가 차지하는 비중이 94.4% ~ 98.9% 정도이고, 이는 LLM이 처리하는 personalized task(또는 단순 task)의 context가 종종 매우 길기 때문이다. 하지만 기존의 연구들은 decoding phase에서의 text generation을 가속화하는 기법들을 주로 다뤘기 때문에(ex. speculative decoding 등), 본 연구에서는 on-device LLM의 prefill speed를 확보하는 것을 목표로 한다.

LLM의 prefill phase는 computation-bound하므로, mobile CPU/GPU의 제한된 computing power가 prefill phase에서의 병목이 될 수 있다. 이에 따라 INT(Integer) vector operation에 효율적이고, energy-efficient하고, workload contention이 비교적 적은 NPU를 사용하는 것이 효과적일 수 있다. 하지만 NPU가 LLM inference에 대해 여러 장점을 가지고 있음에도, 상용 mobile NPU에서 LLM inference를 지원하는 시스템은 존재하지 않는다. 그 이유는 아래와 같다.

<!-- matrix-matrix multiplication에서도 NPU가 GPU를 능가할 수 있나? 아니면 vector-matrix multiplication에서만 더 뛰어난건가? -->

- Costly preparation for variable-length prompts

    현재의 mobile NPU는 static computation graph만을 지원한다. 즉, static shape의 prompt만을 처리할 수 있는데, LLM prompt는 dynamic하다는 문제가 있다. 이에 따라 새로운 shape의 prompt가 입력될 때마다 매번 computation graph를 새로 계산하거나 단순히 padding을 추가해 연산할 수 있는데, 두 방식 모두 비효율적이고 비용이 높다.

<!-- mobile NPU가 static graph만을 지원하는 이유를 아케텍처적인 과점에서 다시 찾아보자. -->

- Mismatch between LLM quantization algorithms and mobile NPU design

    outlier activation의 존재 때문에 SOTA LLM들은 per-group quantization을 활용한다. 즉, activation과 weight를 여러 group으로 나누고, group 별 scaling을 통해 outlier가 다른 group에 영향을 주지 못하도록 한다. 하지만 NPU는 per-group MatMul(Matrix Multiplcation)을 수행하지 못하고, 각 group별로 tensor를 쪼개서 각각에 대한 MatMul을 수행한 후 FP(Floating-point) sum operation으로 더해야 한다. 이에 따라 10.7x만큼의 performance overhead가 존재한다.

<!-- outlier activation 때문에 group quantization을 쓴다? 이거에 대한 더 자세한 설명을 찾아보자. -->
<!-- NPU가 group 단위 연산이 불가능한 것은, NPU는 그 아키텍처적인 특성상 단순 곱만을 수행하므로 그룹별 연산을 수행할 순 없기 때문인가.. -->

- FP operations cannot be eliminated

    LLM은 layer normalizatoin, attention 등의 연산을 수행해야 하므로 accuracy 저하 없이 INT-only execution으로 quantization하기 어렵다. 하지만 mobile NPU는 일반적으로 INT MatMul은 잘 가속화하지만, FP MatMul은 잘 하지 못한다.

<!-- mobile NPU는 왜 INT에 적합하고 FP 연산은 잘 못하는 것인가? 그냥 아키텍처가 그렇게 생겨먹은건가.. 더 찾아보자. -->
<!-- 상용 NPU 스펙을 봐보는 것도 좋겠다. -->

### llm.npu

논문에서 제안하는 llm.npu는 효율적인 NPU offloading을 구현한 최초의 LLM inference system으로, mobile-sized decoder-only LLM에 대해 prefill latency와 energy consumption을 줄이는 것을 그 목표로 한다. llm.npu의 핵심 아이디어는 NPU offloading으로 INT computation을 가속화하고, FP 연산은 CPU/GPU에서 수행하여 accuracy를 유지하는 것이다. 이를 위해 llm.npu는 prompt와 모델에 대해 prompt/tensor/block level 각각에 대한 re-constructing을 수행한다.

- Chunck-sharing graph execution (prompt level re-constructing)

    variable-length prompt들을 다수의 fixed-sized chunk들로 나눠 연산한다. 이때 chunck는 그 크기에 따라 대응되는 pre-built된 subgraph를 가지고 있어 NPU에서 연산이 가능하다. 특히 각 chunk는 chunk-level causal dependency를 고려하여 실행되는데, 이는 선행 token들만을 고려해 attention 연산을 수행하는 decoder의 insight를 활용한 것으로, data dependancy를 유지한다.

    pre-built chunk graph들을 동시에 load하는 것은 memory overhead가 크므로, prompt size에 관련 없는 연산(ex. FFN)들은 각 chunk의 실행에서 공유하도록 한다.

<!-- 어떻게 causal하게 처리한다는 것인가? -->
<!-- memory에 load하는 것을 말하는 건가? -->
<!-- 이게 왜 prompt size에 관련 없다는 것인가? -->

- Shadow outlier execution (tensor level re-constructing)

    outlier들은 추출해 CPU/GPU에서 병렬적으로 연산하도록 한다.

    이때 outlier들에 대한 MatMul을 위해 weight를 CPU memory에 복사함에 따라 증가하는 memory footprint와, CPU/GPU와 NPU 사이의 synchronization overhead가 발생한다. 이에 따라 outlier들이 특정 channel position에서 주로 등장한다는 관찰을 기반으로, 해당 channel들에 해당하는 weight들만 memory에 올려두고 나머지는 disk에 저장했다가 on demand로 가져오도록 한다. 또한 outlier의 importance를 측정하고 그 값에 따라 prune하여 synchronization overhead를 줄인다.

    이때 outlier들은 굉장히 sparse하다고(0.1% ~ 0.3%) 한다.

<!-- 기본이 NPU이고, NPU에서의 outlier들을 추출해 GPU에서 연산하는 것? -->
<!-- CPU memory에 왜 복사하지? 어차피 다 하나의 메모리 안에 있는 거 아닌가? 병렬적으로 처리하기 때문인가? 모델 전체를 올려놓는 게 아니므로.. -->
<!-- memory footprint는 메모리 사용량을 말하는 거 같다. -->
<!-- 여기에서 channel은 뭘 말하는 건가? -->
<!-- 이걸 써서 group quantization을 하지 않는건가..? -->

- Out-of-order subgraph execution (block level re-constructing)

    transformer block들을 out-of-order로 CPU/GPU와 NPU에 scheduling한다. 다시 말해, 각 chunk에 대한 subgraph들을 기존 prompt에서의 순서로 딱 맞춰 실행하는 대신, out-of-order로 scheduling한다. 
    
    이때 subgraph들에 대한 최적의 실행 순서를 찾는 것은 NP-hard이므로, llm.npu는 microsecond-level의 online scheduling 알고리즘을 사용한다. 이는 NPU의 연산이 더 heavy하며, critical path를 구성하기 때문에, 각 subgraph의 실행이 가지는 latency를 줄이는 것보다는 NPU stall이 적어지도록 하는 것을 우선적으로 선택한다. 즉, 이 기법은 parallel processing을 극대화하는 것보다는, CPU/GPU의 float 연산에 따른 bubble을 줄여 CPU/GPU의 영향을 없애면서 NPU 활용을 늘리는 데에 목적이 있다.

<!-- critical path란? -->
<!-- NP-hard란? -->

llm.npu는 MLLM과 QNN을 활용해 구현되었고, 여러 모델과 device, benchmark에 대해 실험한 결과 inference accuracy를 유지하면서 prefill latency과 energy consumption 측면에서 모든 baseline에 대해 성능 향상을 보였다고 한다.

## Background

### On-device LLM Inference Analysis

Opt, Gemma, Qwen, phi 등 on-device에서 돌아갈 수 있을만한 여러 lightweight LLM들이 개발되어 있지만, inference latency는 여전히 무시하지 못할 수준이다. 논문에서는 이 inference latency에 대한 관찰을 수행했다. 지금까지 decoding phase에 대한 연구가 활발히 진행된 것에 반해, 아래 그래프에서와 같은 전형적인 mobile application들에서 prefill phase가 병목인 것을 알 수 있다. 수준의 차이는 있지만 CPU를 활용하는 경우와 GPU를 활용하는 경우 모두에서 prefill latency의 비중이 높았고, prompt length가 길어질수록 그 비율이 높아졌다고 한다.

![](/assets/img/posts/2025-08-09-llm-npu/prefill latency.png){: width="450" }

이런 application들에서 prefill phase가 병목이 되는 이유는 아래와 같다.

- mobile CPU/GPU는 application logic이나 redering task를 처리하는 것을 주 목적으로 하므로, cloud GPU보다 parallelism capability가 낮다.

- mobile LLM task는 personalized, context-aware generation을 위해 long prompt에 대한 처리를 수행하므로 prefill latency가 큰 반면, output length가 대체로 짧아 decoding latency가 적다. 예를 들어, automated email reply의 경우 다양한 사용자 데이터를 고려해야 하고, UI automation의 경우 HTML 파일들과 사용자 명령을 고려해야 한다. 

### Mobile NPUs

이런 prefill latency를 줄이기 위해, 저자는 mobile SoC(System on Chip)에 포함된 NPU(Neural Processing Unit)를 활용하는 것을 제안한다. NPU는 특히 INT8과 같은 INT-based MatMul 연산에 적합하다고 한다. 아래는 주요 제조사들이 제공하는 NPU 목록이다.

![](/assets/img/posts/2025-08-09-llm-npu/npu vendors.png){: width="450" }

mobile NPU는 SIMD(Single Instruction Multiple Data) architecture를 활용해 성능 향상을 달성하고, 특정 상황(clock frequency 500MHz ~ 750MHz)에서 mobile CPU/GPU보다 energy-efficient하다. 또한 별도의 physical memory를 가지는 cloud GPU와 달리, SoC의 mobile NPU는 CPU와 physical memory를 공유하므로 별도의 memory copying이 필요하지 않다.

아래 표와 같이 mobile LLM이 주로 수행하는 matrix size에 대한 MatMul 성능을 Redmi K70 Pro에서 실험한 결과, NPU INT8 성능은 CPU INT8, GPU FP16, NPU FP16보다 성능이 뛰어남을 알 수 있다. 즉, NPU의 INT8 SIMD architecture에 의해 NPU는 INT8 MatMul에 대해 최적의 성능을 보인다.

![](/assets/img/posts/2025-08-09-llm-npu/npu matmul.png){: width="450" }

mobile NPU에서 DNN을 돌리는 과정에서는 아래 그림과 같이 NPU environment setting, graph building, graph optimizing, graph executing이 수행되는데, 이 중 특히 graph building과 graph optimizing의 latency가 크다. graph building에서는 모델을 NPU-required intermediate representation으로 변환하고 memory를 할당하는 작업 등이 수행되고, graph optimizing에서는 memory layout 및 execution order 결정, operator fusion 등이 수행된다.

![](/assets/img/posts/2025-08-09-llm-npu/npu graph.png){: width="450" }

<!-- NPU가 FP가 아니라 INT 연산에서 강점을 가지는 이유? 아키텍처적인 이유가 있을 것 같다. -->
<!-- 여기서는 heteroLLM과 달리 INT 연산이 훨씬 좋다고 한다. INT 연산이 빠른 건 알겠는데, 정확도는 보여주고 있지 않다. -->
<!-- clock frequency에 따라 NPU의 efficiency가 달라지나? -->

### Gap between LLMs and Mobile NPUs

현존하는 DNN engine 중에 mobile NPU를 활용한 LLM acceleration을 지원하는 것은 없었는데, 이는 아래와 같이 mobile NPU와 LLM inference pipeline 사이의 간극이 존재하기 때문이라고 한다.

<!-- 그럼 QNN은 뭐냐? -->

- LLM의 variable-length prompt

    앞서 살펴본 것처럼 mobile NPU를 활용하려면 우선 computation graph를 build 및 optimize해야 한다. 반면 LLM prefill phase에서는 variable-length prompt를 처리하는 것이 필수적이므로, mobile NPU를 단순 활용하는 경우 각 prompt에 대해 graph를 build 및 optimize하는 overhead(ms도 아니고 s 수준의 latency)가 존재하므로 CPU 또는 그 이상으로 느려진다.

<!-- NPU가 static한 graph만을 활용하는 더 구체적인 이유가 궁금하다. -->

- Activation outlier들에 의한 quantization 적용

    아래 그래프(Figure 4)에서 확인할 수 있듯이, NPU를 활용하는 경우 accuracy 유지를 위해 group quantization을 적용하는 K-Quant, AWQ에서는 latency overhead가 크다. 이는 group quantization의 경우 각 group 별 scaling을 적용하므로 group 별 MatMul을 수행해야 하고, 연산 이후 각 group에 대한 결과를 FP operation을 사용해 aggregate해야 하기 때문이다(Figure 3). 이 경우 NPU의 computation capability를 충분히 활용하지 못한다.

    반면 per-tensor quantization을 적용하는 SmoothQuant에서는 3.9% ~ 8.4%의 accuracy 저하가 발생하고, 단순 per-tensor quantization을 적용하는 경우 심각한 accuracy 저하가 발생한다.

![](/assets/img/posts/2025-08-09-llm-npu/NPU vs LLM 2.png){: width="450" }

<!-- 애초에 모델 성능이 낮은 듯하다. -->

- On-device LLM inference에서의 FP operation 활용

    quantization을 적용한 전형적인 LLM workflow는 아래 그림과 같다(Figure 5). accuracy 유지를 위해, linear layer에 대해서는 INT MatMul을 수행하고, attention과 layernorm 등 기타 부분들에서는 FP operation을 수행한다(Table 4). 즉, NPU에서 해당 연산들이 효율적으로 수행되기 어렵다.

![](/assets/img/posts/2025-08-09-llm-npu/NPU vs LLM 3.png){: width="450" }

<!-- INT MatMul에서는 연산 전에 activation을 INT로 변환하고 weight와 연산한다. weight는 이미 INT인 듯하다. -->

## llm.npu Design

앞서 언급한 것처럼, llm.npu는 mobile-sized decoder-only LLM에 대해 prefill latency와 energy consumption을 줄이는 것을 그 목표로 한다. llm.npu의 핵심 아이디어는 NPU offloading으로 INT computation을 가속화하고, FP 연산은 CPU/GPU에서 수행하여 accuracy를 유지하는 것이다. 이를 위해 llm.npu는 prompt와 모델에 대해 prompt/tensor/block level 각각에 대한 re-constructing을 수행한다.

llm.npu의 동작은 perparation stage와 execution stage로 나누어 볼 수 있다.

- Perparation stage

    llm.npu는 enhanced per-tensor quantization을 적용한다. 즉, W8A8로 per-tensor quantization을 적용하는데, activation outlier들을 추출해 독립된 연산을 수행하도록 한다.

    fixed-length chunk-sharing graph들을 생성한다.

<!-- 이건 weight와 activation 둘 다 INT로 quantize한다는 것인가? -->
<!-- 근데 이런 outlier 추출은 offline으로 수행되는 건 아닐 거 같은데.. perparation이 맞나. 일단 더 읽어보자. -->

- Execution stage

    prompt를 입력받으면, llm.npu은 이를 fixed-sized chunk들로 나누고 각각을 causal하게 연산한다. 이때 각 chunk graph들은 다시 subgraph들로 나뉘어지고, data format(INT or FP)에 따라 CPU/GPU와 NPU에 scheduling된다.

    linear layer는 기본적으로 NPU에서 INT8로 연산되고, outlier에 대해서는 shadow execution이 적용되어 CPU/GPU에서 FP로 병렬 연산된다.

    또한 실행 시에 각 chunk들은 out-of-order로 scheduling된다.

### Chunk-sharing graph execution

- Chunk-wise Prefill

    dynamic prompt length 문제를 해결하기 위해 단순히 padding을 붙일 수도 있지만, 이 경우 flexibility 및 scalability가 떨어지고, 또한 padding 연산에 따른 compute resource 낭비가 발생할 수 있다.

    이에 따라 논문에서는 decoder-only 모델 등에서 token들을 causal하게 연산하는 데에서 착안하여, 아래 그림과 같이 prompt를 특정 길이의 chunk들로 나눠 causal하게 연산하고, 남은 부분에는 padding을 추가해 연산한다. 즉, 어떤 chunk에서 attention 연산을 수행할 때는 앞쪽 chunk 모두를 포함시켜 연산한다.

    preparation stage에서는 fixed-length의 computation graph들을 pre-build 및 pre-optimize하고, execution stage에서는 prompt를 여러 개의 chunk로 나누고 대응되는 graph들을 활용해 연산한다.

    이때 연산이 causal하게 수행되므로 각 chunk는 다른 length의 attention 연산을 처리해야 하고, 개별적인 computation graph를 가져야 한다. 이에 따라 단순히 chunk-wise하게 처리한다면 memory overhead가 크다. 그래서 논문에서는 더 나아가 chunk-sharing graph를 제안한다.

![](/assets/img/posts/2025-08-09-llm-npu/chunk sharing graph.png){: width="450" }

- Chunk-sharing Graph

    LLM의 연산은 sequence(chunk) length에 독립적으로 수행되어 chunk별로 공유가 가능한 Static Operator들과(ex. linear, layer normalization), sequence length에 종속적으로 수행되어 공유가 불가능한 Dynamic Operator들이(ex. attention) 있다. 이에 따라 llm.npu에서는 LLM을 static operator들과 dynamic operator들로 나눠, static operator들에 대응되는 subgraph는 한 번만 build 및 optimize하고, dynamic operator들에 대응되는 subgraph는 chunk 마다 개별적으로 build 및 optimize한다. 즉, activation에 대해서 매번 동일한 static operator의 subgraph가 사용되고, dynamic operator에 대해선 적절한 sub-graph가 동적으로 선택된다.

    linear layer는 입력 tensor size에 독립적으로 계산되고, layer normalization은 token 내부 값들에 대한 normalization이므로 sequence length에 독립적으로 연산된다.

    이에 따라 memory overhead가 크게 줄어들고, scalability가 확보된다. 특히 dynamic operator 중 대부분을 차지하는 attention은 weight를 포함하지 않고 activation buffer만 포함하므로 memory 사용량이 적다.

    논문에서는 특정 환경에서 memory consumption을 75%까지 줄였다고 한다. 그리고 아래 그림과 같이 Xiaomi 14에 대해 Qwen1.5-1.8B와 Gemma-2B로 실험한 결과 최적의 chunk length는 256이었다고 한다. 물론 이런 profiling은 NPU별로 수행되어야 한다.

![](/assets/img/posts/2025-08-09-llm-npu/optimal chunk length.png){: width="450" }

<!-- 이 그림은 computation graph를 나타낸 것이다. 실제로 QKV linear layer가 별도로 존재하는 게 아닐 것. -->
<!-- 당연하게도 연산 구조는 같고 값만 다르다면 동일한 graph를 쓸 수 있을 것이다. 근데 graph는 곱하는 값도 포함한다고 했는데... 어케되는거지. weight만 포함한다면? weight가 달라지면 graph도 달라지나? 코드를 봐야 할 것 같기도 하다. -->
<!-- graph 저장할 때 weight 등 곱하는 값도 저장되는건가? 그래서 memory overhead가 크다는 거..? 그런 것 같다. -->
<!-- 생성과 최적화 모두 한 번만 한다. 이렇게 했을 때 더 높은 수준의 최적화가 어려워질지도 모르겠다. 딱히 차이가 없을 수도 있고. -->
<!-- 근데 chunking을 적용하는 것이 적용하지 않는 것에 비해 더 좋은 것인지 모르겠다. 그래프만 보면 chunk length가 길어져도 latency가 크게 높아지지 않는데.. -->








### Shadow outlier execution

### Out-of-order subgraph execution

## Implementation & Evaluation

## Discussion & Future Work

## 결론


