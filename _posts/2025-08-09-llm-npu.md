---
title: "Fast On-device LLM Inference with NPUs"
date: 2025-08-09 14:18:00 +0900
categories: [Papers, Inference Acceleration]
tags: [cs, ai, on-device ai]
math: true
---

해당 논문([링크](https://dl.acm.org/doi/pdf/10.1145/3669940.3707239))은 ASPLOS 2025에 publish된 논문으로, 이전에 정리한 HeteroLLM 다음으로 읽어보게 되었다. HeteroLLM은 method 측면에서는 흥미로웠지만 논리적인 허점이 존재하고, 학회에 accpet되어 있지도 않았으며, 코드도 배포되어 있지 않았다.
반면 이 논문은 최근 메이저 학회에 accept되었으면서 코드([UbiquitousLearning/mllm](https://github.com/UbiquitousLearning/mllm?tab=readme-ov-file))도 배포되어 있기 때문에 교수님께서 다음으로 읽어볼 논문으로 추천해주셨다. 심지어 github 외에도, 해당 코드에 대한 활용 가이드([mllm_website](https://ubiquitouslearning.github.io/mllm_website/#features))도 제공하고 있다.

## Abstract

privacy 문제와 mobile-sized model들의 등장에 따라, LLM의 on-device inference는 그 중요성이 커지고 있다. 하지만 특히 prefill stage에서의 inference latency가 그 병목으로 존재하는데, 본 논문에서 제안하는 llm.npu에서는 NPU offloading을 활용해 prefill latency를 줄인다. llm.npu는 prompt와 모델에 대해 prompt level, tensor level, block level 각각에 대한 re-constructing으로 효율적인 이를 달성한다.

## Introduction

### 해당 연구의 필요성

Qwen2와 Phi3와 같이 적은 parameter를 가진 mobile-sized 모델들이 좋은 성능을 보이지만, 높은 inference latency는 여전히 큰 장애물로 남아있다. 실험을 통해 확인해보면 end-to-end latency 중 prefill stage가 차지하는 비중이 94.4% ~ 98.9% 정도이고, 이는 LLM이 처리하는 personalized task(또는 단순 task)의 context가 종종 매우 길기 때문이다. 하지만 기존의 연구들은 decoding stage에서의 text generation을 가속화하는 기법들을 주로 다뤘기 때문에(ex. speculative decoding 등), 본 연구에서는 on-device LLM의 prefill speed를 확보하는 것을 목표로 한다.

LLM의 prefill stage는 computation-bound하므로, mobile CPU/GPU의 제한된 computing power가 prefill stage에서의 병목이 될 수 있다. 이에 따라 integer vector operation에 효율적이고, energy-efficient하고, workload contention이 비교적 적은 NPU를 사용하는 것이 효과적일 수 있다. 하지만 NPU가 LLM inference에 대해 여러 장점을 가지고 있음에도, 상용 mobile NPU에서 LLM inference를 지원하는 시스템은 존재하지 않는다. 그 이유는 아래와 같다.

<!-- matrix-matrix multiplication에서도 NPU가 GPU를 능가할 수 있나? 아니면 vector-matrix multiplication에서만 더 뛰어난건가? -->

- Costly preparation for variable-length prompts

    현재의 mobile NPU는 static computation graph만을 지원한다. 즉, static shape의 prompt만을 처리할 수 있는데, LLM prompt는 dynamic하다는 문제가 있다. 이에 따라 새로운 shape의 prompt가 입력될 때마다 매번 computation graph를 새로 계산하거나 단순히 padding을 추가해 연산할 수 있는데, 두 방식 모두 비효율적이고 비용이 높다.

<!-- mobile NPU가 static graph만을 지원하는 이유를 아케텍처적인 과점에서 다시 찾아보자. -->

- Mismatch between LLM quantization algorithms and mobile NPU design

    outlier activation의 존재 때문에 SOTA LLM들은 per-group quantization을 활용한다. 즉, activation과 weight를 여러 group으로 나누고, group 별 scaling을 통해 outlier가 다른 group에 영향을 주지 못하도록 한다. 하지만 NPU는 per-group MatMul(Matrix Multiplcation)을 수행하지 못하고, 각 group별로 tensor를 쪼개서 각각에 대한 MatMul을 수행한 후 float sum operation으로 더해야 한다. 이에 따라 10.7x만큼의 performance overhead가 존재한다.

<!-- outlier activation 때문에 group quantization을 쓴다? 이거에 대한 더 자세한 설명을 찾아보자. -->
<!-- NPU가 group 단위 연산이 불가능한 것은, NPU는 그 아키텍처적인 특성상 단순 곱만을 수행하므로 그룹별 연산을 수행할 순 없기 때문인가.. -->

- Floating point opeerations cannot be eliminated

    LLM은 layer normalizatoin, attention 등의 연산을 수행해야 하므로 accuracy 저하 없이 integer-only execution으로 quantization하기 어렵다. 하지만 mobile NPU는 일반적으로 integer MatMul은 잘 가속화하지만, floating point MatMul은 잘 하지 못한다.

<!-- mobile NPU는 왜 integer에 적합하고 floating point 연산은 잘 못하는 것인가? 그냥 아키텍처가 그렇게 생겨먹은건가.. 더 찾아보자. -->
<!-- 상용 NPU 스펙을 봐보는 것도 좋겠다. -->

### llm.npu

논문에서 제안하는 llm.npu는 효율적인 NPU offloading을 구현한 최초의 LLM inference system으로, decoder-only LLM(ex. llama, gpt 등)에 대해 prefill latency와 energy consumption을 줄이는 것을 그 목표로 한다. llm.npu의 핵심 아이디어는 NPU를 활용해 integer computation을 가속화하고, 필수적인 floating point 연산은 CPU/GPU에서 수행하여 accuracy를 확보하는 것이다. 이를 위해 llm.npu는 아래와 같이 promp와 모델에 대해 prompt/tensor/block level로 re-constructing을 수행한다.

- Chunck-sharing graphs (prompt level re-constructing)

    variable-length prompt들을 다수의 fixed-sized chunk들로 나눠 연산한다. 이때 chunck는 그 크기에 따라 대응되는 pre-built된 subgraph를 가지고 있어 NPU에서 연산이 가능하다. 특히 각 chunk는 chunk-level causal dependency를 고려하여 실행되는데, 이는 선행 token들만을 고려해 attention 연산을 수행하는 decoder의 insight를 활용한 것으로, data dependancy를 유지한다.

    pre-built chunk graph들을 동시에 load하는 것은 memory overhead가 크므로, prompt size에 관련 없는 연산(ex. FFN)들은 각 chunk의 실행에서 공유하도록 한다.

<!-- 어떻게 causal하게 처리한다는 것인가? -->
<!-- memory에 load하는 것을 말하는 건가? -->
<!-- 이게 왜 prompt size에 관련 없다는 것인가? -->

- Shadow outlier execution (tensor level re-constructing)

    outlier들은 추출해 CPU/GPU에서 병렬적으로 연산하도록 한다.

    이때 outlier들에 대한 MatMul을 위해 weight를 CPU memory에 복사함에 따라 증가하는 memory footprint와, CPU/GPU와 NPU 사이의 synchronization overhead가 발생한다. 이에 따라 outlier들이 특정 channel position에서 주로 등장한다는 관찰을 기반으로, 해당 channel들에 해당하는 weight들만 memory에 올려두고 나머지는 disk에 저장했다가 on demand로 가져오도록 한다. 또한 outlier의 importance를 측정하고 그 값에 따라 prune하여 synchronization overhead를 줄인다.

    이때 outlier들은 굉장히 sparse하다고(0.1% ~ 0.3%) 한다.

<!-- 기본이 NPU이고, NPU에서의 outlier들을 추출해 GPU에서 연산하는 것? -->
<!-- CPU memory에 왜 복사하지? 어차피 다 하나의 메모리 안에 있는 거 아닌가? 병렬적으로 처리하기 때문인가? 모델 전체를 올려놓는 게 아니므로.. -->
<!-- memory footprint는 메모리 사용량을 말하는 거 같다. -->
<!-- 여기에서 channel은 뭘 말하는 건가? -->
<!-- 이걸 써서 group quantization을 하지 않는건가..? -->

- Out-of-order subgraph execution (block level re-constructing)

    transformer block들을 out-of-order로 CPU/GPU와 NPU에 scheduling한다. 다시 말해, 각 chunk에 대한 subgraph들을 기존 prompt에서의 순서로 딱 맞춰 실행하는 대신, out-of-order로 scheduling한다. 
    
    이때 최적의 실행 순서를 찾는 것은 NP-hard이므로, llm.npu는 microsecond-level의 online scheduling 알고리즘을 사용한다.
    각 subgraph의 실행이 가지는 latency를 줄이는 것 대신, NPU stall이 적어지도록 하는 것을 우선적으로 선택한다.
    이에 따라

    CPU/GPU의 float 연산에 따른 bubble을 줄인다.


<!-- critical path란? -->
<!-- NP-hard란? -->

## Background

## llm.npu Design

## Implementation & Evaluation

## Discussion & Future Work

## 결론


